{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4440a7b5-db42-4134-ba05-937f1c264bd0",
   "metadata": {},
   "source": [
    "EXPERIMENT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2ec5a-c728-4d01-8978-49979dc75a79",
   "metadata": {},
   "source": [
    "OBJECTIVE : WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a224d6-4f4a-4b77-b386-bf0472b51ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c9884a-3514-4287-b12f-88166cee432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4929, Training Accuracy: 0.9368\n",
      "Epoch 2, Loss: 0.1958, Training Accuracy: 0.9584\n",
      "Epoch 3, Loss: 0.1427, Training Accuracy: 0.9664\n",
      "Epoch 4, Loss: 0.1162, Training Accuracy: 0.9708\n",
      "Epoch 5, Loss: 0.1018, Training Accuracy: 0.9729\n",
      "Epoch 6, Loss: 0.0868, Training Accuracy: 0.9773\n",
      "Epoch 7, Loss: 0.0762, Training Accuracy: 0.9778\n",
      "Epoch 8, Loss: 0.0725, Training Accuracy: 0.9819\n",
      "Epoch 9, Loss: 0.0658, Training Accuracy: 0.9811\n",
      "Epoch 10, Loss: 0.0619, Training Accuracy: 0.9864\n",
      "Test Accuracy: 0.9684\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    image = tf.reshape(image, [-1])  # Flatten to 784\n",
    "    label = tf.one_hot(label, depth=10)  # Convert to one-hot encoding\n",
    "    return image, label\n",
    "\n",
    "# Load dataset and apply preprocessing\n",
    "mnist_dataset = tfds.load(\"mnist\", split=[\"train\", \"test\"], as_supervised=True)\n",
    "train_data = mnist_dataset[0].map(preprocess).shuffle(10000).batch(100)\n",
    "test_data = mnist_dataset[1].map(preprocess).batch(100)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 784\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_size, hidden_layer1_size]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([hidden_layer1_size, hidden_layer2_size]))\n",
    "b2 = tf.Variable(tf.zeros([hidden_layer2_size]))\n",
    "\n",
    "W_out = tf.Variable(tf.random.normal([hidden_layer2_size, output_size]))\n",
    "b_out = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(X):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    output_layer = tf.matmul(layer2, W_out) + b_out  # No activation (logits)\n",
    "    return output_layer\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Training step function\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward_pass(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W_out, b_out])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W_out, b_out]))\n",
    "    return loss\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_accuracy(dataset):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for X, Y in dataset:\n",
    "        logits = forward_pass(X)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "        total_correct += tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "        total_samples += X.shape[0]\n",
    "    return total_correct / total_samples\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_x, batch_y in train_data:\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "        avg_loss += loss\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss /= total_batches\n",
    "    train_acc = compute_accuracy(train_data)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_acc = compute_accuracy(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532749b1-e4d4-4240-b078-dbc6dfbe0c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
