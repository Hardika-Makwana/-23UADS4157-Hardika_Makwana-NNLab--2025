{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4440a7b5-db42-4134-ba05-937f1c264bd0",
   "metadata": {},
   "source": [
    "EXPERIMENT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2ec5a-c728-4d01-8978-49979dc75a79",
   "metadata": {},
   "source": [
    "OBJECTIVE : WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca69333-3baa-4dd7-9d5e-8fa0fb4f8576",
   "metadata": {},
   "source": [
    "DESCRIPTION OF MODEL\n",
    "Model is a fully connected (feedforward) neural network with three layers, designed to classify handwritten digits (0-9) \n",
    "from the MNIST dataset.\n",
    "\n",
    "Model architecture\n",
    "- Input layer : Each MNIST image is 28 × 28 pixels (784 pixels)--> 784 input size\n",
    "- Hidden Layer 1 : 128 neurons , Activation Function: Sigmoid\n",
    "- Hidden Layer 2 : 64 neurons ,Activation Function: Sigmoid\n",
    "- Output Layer: 10 neurons (For each digit from 0 to 9) , No activation because it will be handled by softmax in the loss function.\n",
    "\n",
    "Model parameters(hyperparameters)\n",
    "- Number of epochs : 10\n",
    "- Learning rate : 0.01\n",
    "- Batch size : 100\n",
    "- Loss function : Softmax Cross-Entropy\n",
    "- Optimiser : Adam\n",
    "\n",
    "-Model follows a structured approach using forward propagation, loss calculation, backpropagation, and optimization\n",
    "to learn the correct classification.\n",
    "\n",
    "PYTHON IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c9884a-3514-4287-b12f-88166cee432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4513, Training Accuracy: 0.9366\n",
      "Epoch 2, Loss: 0.1944, Training Accuracy: 0.9579\n",
      "Epoch 3, Loss: 0.1431, Training Accuracy: 0.9670\n",
      "Epoch 4, Loss: 0.1221, Training Accuracy: 0.9692\n",
      "Epoch 5, Loss: 0.1043, Training Accuracy: 0.9718\n",
      "Epoch 6, Loss: 0.0924, Training Accuracy: 0.9775\n",
      "Epoch 7, Loss: 0.0797, Training Accuracy: 0.9785\n",
      "Epoch 8, Loss: 0.0768, Training Accuracy: 0.9791\n",
      "Epoch 9, Loss: 0.0732, Training Accuracy: 0.9760\n",
      "Epoch 10, Loss: 0.0730, Training Accuracy: 0.9844\n",
      "Test Accuracy: 0.9655\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    image = tf.reshape(image, [-1])  # Flatten to 784\n",
    "    label = tf.one_hot(label, depth=10)  # Convert to one-hot encoding\n",
    "    return image, label\n",
    "\n",
    "# Load dataset and apply preprocessing\n",
    "mnist_dataset = tfds.load(\"mnist\", split=[\"train\", \"test\"], as_supervised=True)\n",
    "train_data = mnist_dataset[0].map(preprocess).shuffle(10000).batch(100)\n",
    "test_data = mnist_dataset[1].map(preprocess).batch(100)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 784\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_size, hidden_layer1_size]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([hidden_layer1_size, hidden_layer2_size]))\n",
    "b2 = tf.Variable(tf.zeros([hidden_layer2_size]))\n",
    "\n",
    "W_out = tf.Variable(tf.random.normal([hidden_layer2_size, output_size]))\n",
    "b_out = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(X):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    output_layer = tf.matmul(layer2, W_out) + b_out  # No activation (logits)\n",
    "    return output_layer\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Training step function\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward_pass(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W_out, b_out])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W_out, b_out]))\n",
    "    return loss\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_accuracy(dataset):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for X, Y in dataset:\n",
    "        logits = forward_pass(X)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "        total_correct += tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "        total_samples += X.shape[0]\n",
    "    return total_correct / total_samples\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_x, batch_y in train_data:\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "        avg_loss += loss\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss /= total_batches\n",
    "    train_acc = compute_accuracy(train_data)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_acc = compute_accuracy(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7f887-48cd-426c-846c-f0b67e0d22ae",
   "metadata": {},
   "source": [
    "DESCRIPTION OF CODE\n",
    "- We are using TensorFlow . TensorFlow  is an open-source machine learning library developed by Google. It is used to build and train AI models, especially deep learning models like neural networks.\n",
    "- Imported tensorflow , tensorflow_datasets : Helps in loading prebuilt datasets like MNIST.\n",
    "\n",
    "def preprocess() -->\n",
    "- Dataset is loaded and preprocessing is done . Pixel values are normalised between [0,1] .\n",
    "- Dataset is converted to a 1D array .\n",
    "- One hot encoding is done to convert labels to binary format.\n",
    "- mnist dataset is split into train and test dataset each containing tuple (image , label) , both are TensorFlow dataset objects.\n",
    "- map(preprocess) : Applies preprocessing to every image in the dataset.\n",
    "- shuffle(10000) : Randomly shuffles 10,000 images to improve model generalization.\n",
    "- batch(100) : Divides dataset into mini-batches of 100 for training. For 100 forward passes , backward propagation will be performed once.\n",
    "\n",
    "Neural network parameters are defined : \n",
    "- input_size = 784\n",
    "- hidden_layer1_size = 128\n",
    "- hidden_layer2_size = 64\n",
    "- output_size = 10\n",
    "- learning_rate = 0.01\n",
    "- epochs = 10\n",
    "\n",
    "Weight and bias initialisation :\n",
    "- Weights (W1, W2, W_out) : Randomly initialized\n",
    "- Biases (b1, b2, b_out) : Initialized as zero\n",
    "\n",
    "def forward_pass() -->\n",
    "- Layer 1 output : A1 = sigmoid(XW1 + b1)\n",
    "- Layer 2 output : A2 = sigmoid(A1W2 + b2)\n",
    "- Output Layer (Logits): Output(Z) = (A2W_out + b_out)\n",
    "- No activation in the last layer because it will be handled by softmax in the loss function.\n",
    "\n",
    "def loss_fn() -->\n",
    "- Softmax Cross-Entropy Loss: Measures how different the predictions are from true labels.\n",
    "- Loss = −∑ (Yi) log(softmax(Zi))\n",
    "- tf.reduce_mean(): Computes average loss across all batches.\n",
    "\n",
    "Adam Optimizer : Optimizer adjusts the weights and biases to minimize the loss function during training.\n",
    "\n",
    "def train_step() -->\n",
    "- tf.GradientTape(): Computes gradients automatically.\n",
    "- optimizer.apply_gradients(): Updates weights using the calculated gradients.\n",
    "\n",
    "def compute_accuracy() --> \n",
    "- Predictions (argmax): Returns the index of the highest probability class.\n",
    "- Correct Predictions (equal()): Compares predictions with true labels.\n",
    "- Function helps compute accuracy : correct predictions/total predictions .\n",
    "\n",
    "Training loop :\n",
    "- Loops over 10 epochs.\n",
    "- Calls train_step() on each batch.\n",
    "- Computes training loss and accuracy.\n",
    "\n",
    "Finally the test accuracy has been calculated which comes out to be 96.55% .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10803e7b-a084-49aa-afd8-3ff4b3f6f50c",
   "metadata": {},
   "source": [
    "MY COMMENTS(limitations and scope of improvement)\n",
    "-  The model has been trained to classify the mnist dataset and achived a training accuracy of 98.44% and final test accuracy of 96.55%.\n",
    "-  Accuracy could be improved by using an activation function other than Sigmoid as it is prone to vanishing gardient. ReLU activation\n",
    "  function could have been used.\n",
    "- Accuracy could have been improved by increasing the number of epochs , changing the number of hidden neurons , using other optimiser\n",
    "   or another loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
